#!/usr/local/bin/snakemake

import sys; sys.path.append('/home/joelamkc/notebooks')
import time

# 
#

#SAMPLES = ['tmp']

RAW_DATA_DIR = '/usersdata/joelamkc/hku_ngs/251113_novaseq90'

SAMPLES = ['A1-Psp-NT-ROSA',
           'A2-Psp-1687-ROSA',
           'A3-bt1-NT-ROSA',
           'A4-bt1-1687-ROSA',
           'A5-bt3-NT-ROSA',
           'A6-bt3-1687-ROSA',
           'B1-Psp-NT-ROSA',
           'B2-Psp-1687-ROSA',
           'B3-bt1-NT-ROSA',
           'B4-bt1-1687-ROSA',
           'B5-bt3-NT-ROSA',
           'B6-bt3-1687-ROSA',
           'C1-Psp-NT-XTEN',
           'C2-Psp-1687-XTEN',
           'C3-T136A-NT-XTEN',
           'C4-T136A-1687-XTEN',
           'D1-Psp-NT-XTEN',
           'D2-Psp-1687-XTEN',
           'D3-T136A-NT-XTEN',
           'D4-T136A-1687-XTEN',
           'E1-Psp-NT-HTT',
           'E2-Psp-cr6-HTT',
           'E3-Psp-cr7-HTT',
           'E4-T136A-NT-HTT',
           'E5-T136A-cr6-HTT',
           'E6-T136A-cr7-HTT',
           'F1-Psp-NT-HTT',
           'F2-Psp-cr6-HTT',
           'F3-Psp-cr7-HTT',
           'F4-T136A-NT-HTT',
           'F5-T136A-cr6-HTT',
           'F6-T136A-cr7-HTT']

LANE = 'L1'
READ = '1'

#-----------------------------

#MAX_DIFF = 5
MIN_QUAL = 30
MIN_QUALIFIED = 50
#ADAPTER_3P_FLANK = 'AGATCGGAAGAGCACACGTCT' # NEB 3p adaptor seq 
ADAPTER_3P_FLANK = 'CTCGGCATGGACGAGCTGTA' #oJL687, 3' end of EGFP
#ADAPTER_5P_FLANK = 'NNNNNNNNTATAGGG' # oKW1424.STRIPE-TSO (after 8 random nucleotide)
ADAPTER_5P_FLANK = 'TGGCTCCGATATCACGCTTCT' # P20P + T (from Twister Ribozyme)
#FIRSTBASE_TO_KEEP = 9        # 8 random nucleotide at 5' end of read 1
LASTBASE_TO_KEEP = 50      # to make 100 nucleotide inserts
#INSERT_LENGTH_TO_KEEP = 100       # to make 100 nucleotide inserts
MIN_INSERT_LEN = 18
#MAX_INSERT_LEN = 26
NUM_POSSIBLE_MAPPING = 1 # Report only one site
#NUM_POSSIBLE_MAPPING = 5

BOWTIE2INDEX = '/usersdata/joelamkc/genome/fig4Index/fig4Index'

# write a log for each run
DATE = time.strftime('%y%m%d')
LOG = 'logs/%s.log' % DATE
logf = open(LOG, 'a')
STAMP = '========== NEW RUN at %s ==========\n' % time.time()
logf.write(STAMP)
logf.close()

rule all:
    input: 'logs/%s.log' % DATE,
            expand('data/{sample}.qc.fastq.gz', sample=SAMPLES),
            expand('data/{sample}.trim5.fastq.gz', sample=SAMPLES),
            expand('data/{sample}.trimlen.fastq.gz', sample=SAMPLES),
            expand('data/{sample}.trim3.fastq.gz', sample=SAMPLES),
            expand('data/{sample}.count.txt', sample=SAMPLES),
            expand('data/{sample}.collapsed.fa.gz', sample=SAMPLES),
            expand('data/{sample}.bt2output.sam', sample=SAMPLES),
            expand('data/{sample}.bt2unmapped.fa', sample=SAMPLES),
            expand('data/{sample}.bt2output.simple.txt', sample=SAMPLES)

rule make_log:
    output: 'logs/%s.log' % DATE
    shell: 'echo "{STAMP}" > {output}'

rule quality_filter:
    output: 'data/{sample}.qc.fastq.gz'
    shell: 'zcat {RAW_DATA_DIR}/{wildcards.sample}_{LANE}_{READ}.fq.gz | \
            fastq_quality_filter - -v -Q33 -q {MIN_QUAL} -p {MIN_QUALIFIED} 2>> {LOG} | \
            gzip -c - > {output}'

rule trim_5adaptor:
    input: 'data/{sample}.qc.fastq.gz'
    output: 'data/{sample}.trim5.fastq.gz'
    shell: 'zcat {input} | \
            cutadapt - -g {ADAPTER_5P_FLANK} --discard-untrimmed 2>> {LOG} | \
            gzip -c - > {output}'

rule length_trimmer:
    input: 'data/{sample}.trim5.fastq.gz'
    output: 'data/{sample}.trimlen.fastq.gz'
    shell: 'zcat {input} | \
            fastx_trimmer - -l {LASTBASE_TO_KEEP} 2>> {LOG} | \
            gzip -c - > {output}'

rule trim_3adaptor:
    input: 'data/{sample}.trimlen.fastq.gz'
    output: 'data/{sample}.trim3.fastq.gz'
    shell: 'zcat {input} | \
            cutadapt - -a {ADAPTER_3P_FLANK} -m {MIN_INSERT_LEN} 2>> {LOG} | \
            gzip -c - > {output}'

rule insert_count:
    input: 'data/{sample}.trim3.fastq.gz'
    output: 'data/{sample}.count.txt'
    shell: 'zcat {input} | \
            grep -A1 "^@" --no-group-separator - | grep -v "^@" - | \
            sort - | \
            uniq -c | \
     	    sort -nr > {output}'

# added on 240309
rule collapsing: 
    input: 'data/{sample}.trim3.fastq.gz'
    output: 'data/{sample}.collapsed.fa.gz'
    shell: 'zcat {input} | \
            fastx_collapser - -Q33 2>> {LOG} | \
            gzip -c - > {output}'

rule bowtie2_mapping:
    input: 'data/{sample}.collapsed.fa.gz'  #bowtie2 can get gziped files as input
    output: mapped = 'data/{sample}.bt2output.sam',
	        unmapped = 'data/{sample}.bt2unmapped.fa'
    params: idx = BOWTIE2INDEX
    shell: 'bowtie2 -f --score-min C,0,-1 --norc -p 12 -k {NUM_POSSIBLE_MAPPING} \
            -U {input} -x {params.idx} -S {output.mapped} --un {output.unmapped} 2>> {LOG}'

    # -f: input is fasta
    # --local: ends might be soft clipped
    #  --very-sensitive-local
    # --score-min C,0,-1: perfect match --> end-to-end
    # --norc: no rev comp strand --> use only forward strand reference sequence
    # -p: threads
    # --score-min: min acceptable alignmemt score; L: -0.6, -0.6 for end-to-end
    # --ma: match bonus; 2 for --local
    # -k: report up to <int> alns per read (for normal sRNAseq, I used 5)
    # -S: output will be sam
    # -U: unpaired input file

# added on 240314
rule make_simple_sam:
    input: 'data/{sample}.bt2output.sam'
    output: 'data/{sample}.bt2output.simple.txt'
    shell: "grep -v '^@' {input} | awk -F' ' '{{print$1, $3, $4, $6, $10}}' - > {output}"
    # remove head
    # get several columns